{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "sys.path.append('..')\n",
    "\n",
    "import random \n",
    "import robust as rb \n",
    "import torch as ch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nff.io.ase_calcs import NeuralFF, AtomsBatch, EnsembleNFF\n",
    "from nff.data import Dataset\n",
    "from nff.train import load_model \n",
    "\n",
    "from ase.io import Trajectory, read \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting xyz to dataset \n",
    "def _xyz_to_dataset(xyz_file):\n",
    "    # read the xyz file\n",
    "    atoms_list = read(xyz_file, index=':')\n",
    "\n",
    "    # initialize the props dictionary \n",
    "    props = {\n",
    "        'nxyz' : [],\n",
    "        'energy' : [],\n",
    "        'energy_grad' : [],\n",
    "        'stress' : [],\n",
    "        'lattice' : [],\n",
    "    }\n",
    "    \n",
    "    for atoms in atoms_list:\n",
    "        # get the atomic number and positions\n",
    "        numbers = atoms.get_atomic_numbers()\n",
    "        positions = atoms.get_positions()\n",
    "        \n",
    "        #get the lattice\n",
    "        lattice = atoms.get_cell().array\n",
    "        props['lattice'].append(lattice)\n",
    "\n",
    "        # combine the atomic numbers and positions \n",
    "        nxyz = np.column_stack((numbers, positions))\n",
    "        props['nxyz'].append(nxyz)\n",
    "\n",
    "        # xtract the energy\n",
    "        energy = atoms.info['REF_energy']\n",
    "        props['energy'].append(energy)\n",
    "        \n",
    "        # get the energy gradient\n",
    "        forces = atoms.todict()['REF_force']\n",
    "        energy_grad = -forces  # Note the negative sign\n",
    "        props['energy_grad'].append(energy_grad)\n",
    "\n",
    "        # Extract stress if available\n",
    "        if atoms.info['REF_stress'].any():\n",
    "            stress = atoms.info['REF_stress']\n",
    "            props['stress'].append(stress)\n",
    "        else:\n",
    "            props['stress'].append(None)\n",
    "        \n",
    "        # create the dataset object\n",
    "        dataset = Dataset(props=props, units='eV')\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from ase.io import read\n",
    "from nff.data import Dataset\n",
    "\n",
    "\n",
    "def xyz_to_dataset(xyz_file):\n",
    "    # Read the XYZ file\n",
    "    atoms_list = read(xyz_file, index=':')\n",
    "    \n",
    "    # Initialize the props dictionary\n",
    "    props = {\n",
    "        'nxyz': [],\n",
    "        'energy': [],\n",
    "        'energy_grad': [],\n",
    "        'stress': [],\n",
    "        'lattice': [],\n",
    "        'num_atoms': []  # Add this to keep track of the number of atoms in each structure\n",
    "    }\n",
    "    \n",
    "    for atoms in atoms_list:\n",
    "        # Extract atomic numbers and positions\n",
    "        numbers = atoms.get_atomic_numbers()\n",
    "        positions = atoms.get_positions()\n",
    "        \n",
    "        # Get the lattice\n",
    "        lattice = atoms.get_cell().array\n",
    "        props['lattice'].append(torch.tensor(lattice, dtype=torch.float32))\n",
    "        \n",
    "        # Combine atomic numbers and positions into nxyz format\n",
    "        nxyz = np.column_stack((numbers, positions))\n",
    "        props['nxyz'].append(torch.tensor(nxyz, dtype=torch.float32))\n",
    "        \n",
    "        # Extract energy\n",
    "        energy = atoms.info['REF_energy']\n",
    "        props['energy'].append(torch.tensor(energy, dtype=torch.float32))\n",
    "        \n",
    "        # Extract forces and convert to energy gradient\n",
    "        forces = atoms.todict()['REF_force']\n",
    "        energy_grad = -forces  # Note the negative sign\n",
    "        props['energy_grad'].append(torch.tensor(energy_grad, dtype=torch.float32))\n",
    "        \n",
    "        # Extract stress if available\n",
    "        if atoms.info['REF_stress'].any():\n",
    "            stress = atoms.info['REF_stress']\n",
    "            props['stress'].append(torch.tensor(stress, dtype=torch.float32))\n",
    "        else:\n",
    "            props['stress'].append(torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=torch.float32))\n",
    "        \n",
    "        # Add the number of atoms in this structure\n",
    "        props['num_atoms'].append(torch.tensor(len(atoms), dtype=torch.int64))\n",
    "    \n",
    "    # Convert single-value properties to tensors\n",
    "    props['energy'] = torch.tensor(props['energy'])\n",
    "    props['stress'] = torch.stack(props['stress'])\n",
    "    props['lattice'] = torch.stack(props['lattice'])\n",
    "    props['num_atoms'] = torch.tensor(props['num_atoms'])\n",
    "    \n",
    "    # Keep multi-dimensional properties as lists of tensors\n",
    "    # props['nxyz'] and props['energy_grad'] are already lists of tensors\n",
    "    \n",
    "    # Create the Dataset object\n",
    "    dataset = Dataset(props=props, units='eV')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(8):\n",
    "    model_path = f'finished_runs/2024-09-27_vcrtiwzr_fep+vac+neb+perf/model_{i}/vcrtiwzr_all_e1_f50_25s_seed_{i}_compiled.model'\n",
    "    m = NeuralFF.from_file(model_path,device='cpu',).model\n",
    "    models.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleNFF(models,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = xyz_to_dataset('data/fep_vac_neb_perf_test.xyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF = 6\n",
    "\n",
    "def get_atoms(props):\n",
    "    atoms = AtomsBatch(\n",
    "        positions=props['nxyz'][:,1:],\n",
    "        numbers=props['nxyz'][:,0],\n",
    "        cell=props['lattice'],\n",
    "        pbc=True,\n",
    "        cutoff=CUTOFF,\n",
    "        props={'energy': 0, 'energy_grad': [], 'stress': []},\n",
    "        calculator=ensemble,\n",
    "        device='cpu',\n",
    "    )\n",
    "    _ = atoms.update_nbr_list()\n",
    "\n",
    "    return atoms \n",
    "\n",
    "initial = get_atoms(random.choice(dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the attacker\n",
    "energies_per_atom = ch.tensor(dset.props['energy']) / dset.props['num_atoms']\n",
    "#forces = ch.tensor(dset.props['energy_grad'])\n",
    "\n",
    "energy_dset = rb.PotentialDataset(\n",
    "    ch.zeros_like(energies_per_atom),\n",
    "    energies_per_atom,\n",
    "    energies_per_atom,\n",
    ")\n",
    "\n",
    "loss_fn = rb.loss.AdvLoss(\n",
    "    train=energy_dset,\n",
    "    temperature=20,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker = rb.schnet.Attacker(\n",
    "    initial,\n",
    "    ensemble,\n",
    "    loss_fn,\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "forward() Expected a value of type 'Dict[str, Tensor]' for argument 'data' but instead found type 'dict'.\nPosition: 1\nValue: {'energy': tensor(0.), 'energy_grad': tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]), 'stress': tensor([0., 0., 0., 0., 0., 0.]), 'num_atoms': tensor([124]), 'nbr_list': tensor([[  0,   1],\n        [  0,   3],\n        [  0,   4],\n        ...,\n        [121, 122],\n        [121, 123],\n        [122, 123]]), 'offsets': tensor(indices=tensor([[    0,     0,     0,  ..., 12900, 12900, 12900],\n                       [    0,     1,     2,  ...,     0,     1,     2]]),\n       values=tensor([ 6.3086, 10.8680, -8.8094,  ...,  6.3086, 10.8680,\n                      -8.8094]),\n       size=(12904, 3), nnz=22692, layout=torch.sparse_coo), 'cell': tensor([[ 1.2612e+01, -3.6813e-02, -4.4531e+00],\n        [-6.3037e+00,  1.0905e+01, -4.3563e+00],\n        [-1.2380e-02,  9.7350e-02,  1.3324e+01]]), 'lattice': tensor([[ 1.2612e+01, -3.6813e-02, -4.4531e+00],\n        [-6.3037e+00,  1.0905e+01, -4.3563e+00],\n        [-1.2380e-02,  9.7350e-02,  1.3324e+01]], dtype=torch.float64), 'nxyz': tensor([[ 4.0000e+01,  7.4146e+00,  8.7504e+00, -2.7836e+00],\n        [ 4.0000e+01,  2.4671e+00,  1.4621e-01,  7.0905e+00],\n        [ 4.0000e+01,  1.2476e+00,  2.3217e+00,  1.1514e+01],\n        [ 4.0000e+01, -1.3462e+00,  6.5356e+00,  4.3810e+00],\n        [ 4.0000e+01, -2.4706e+00,  8.6695e+00,  8.2488e-01],\n        [ 4.0000e+01,  3.7842e+00,  2.2125e+00,  5.8137e-02],\n        [ 4.0000e+01, -2.5721e-02,  8.6543e+00, -2.4998e+00],\n        [ 4.0000e+01,  1.9152e-01,  8.8615e+00,  5.3019e+00],\n        [ 4.0000e+01,  5.0605e+00,  4.4237e+00,  8.8908e+00],\n        [ 4.0000e+01,  4.9347e+00,  8.7714e+00,  9.8331e-01],\n        [ 2.2000e+01,  6.1132e+00,  1.0829e+01, -6.1018e+00],\n        [ 2.2000e+01, -1.2548e+00,  2.3091e+00,  9.7359e+00],\n        [ 2.2000e+01,  8.7753e+00,  6.4054e+00, -1.7335e+00],\n        [ 2.2000e+01, -2.1754e-02,  4.2954e+00,  2.6812e+00],\n        [ 2.2000e+01, -8.2604e-02,  4.4695e+00,  8.0474e+00],\n        [ 2.2000e+01,  5.2318e+00,  1.4455e-01,  6.1814e+00],\n        [ 2.2000e+01,  3.9508e+00,  2.1815e+00,  1.0501e+01],\n        [ 2.2000e+01,  1.2982e+00,  6.5365e+00,  9.8642e-01],\n        [ 2.2000e+01,  7.6722e+00,  1.4307e-01, -2.5230e+00],\n        [ 2.2000e+01,  7.5634e+00,  1.4962e-01,  2.5938e+00],\n        [ 2.2000e+01,  7.4822e+00,  1.4709e-01,  7.8721e+00],\n        [ 2.2000e+01,  5.0839e+00,  4.4377e+00, -1.5865e+00],\n        [ 2.2000e+01,  3.7637e+00,  6.5361e+00,  2.7470e+00],\n        [ 2.2000e+01,  2.9994e+00,  9.3758e+00,  4.8114e+00],\n        [ 2.2000e+01,  8.8663e+00,  2.1270e+00, -4.3513e+00],\n        [ 2.2000e+01,  8.7727e+00,  2.2294e+00,  6.1476e+00],\n        [ 2.2000e+01,  7.5389e+00,  4.3442e+00,  7.1980e-02],\n        [ 2.2000e+01,  5.1560e+00,  8.6788e+00,  3.7544e+00],\n        [ 2.3000e+01,  1.2409e+01,  1.8510e-01,  8.6290e-01],\n        [ 2.3000e+01,  1.2452e+01,  8.3813e-02,  3.6087e+00],\n        [ 2.3000e+01,  6.3092e+00,  1.0939e+01,  1.9948e+00],\n        [ 2.3000e+01,  1.1303e+01,  2.2009e+00, -5.3179e+00],\n        [ 2.3000e+01,  1.1277e+01,  2.1472e+00, -4.8324e-02],\n        [ 2.3000e+01,  1.1282e+01,  2.1815e+00,  2.6107e+00],\n        [ 2.3000e+01,  9.9991e+00,  4.4390e+00,  7.1078e+00],\n        [ 2.3000e+01,  1.0019e+01,  4.1195e+00, -9.1255e-01],\n        [ 2.3000e+01,  1.0034e+01,  4.3643e+00,  1.8698e+00],\n        [ 2.3000e+01,  8.7705e+00,  6.6091e+00,  6.1481e+00],\n        [ 2.3000e+01,  8.7144e+00,  6.2790e+00, -4.5387e+00],\n        [ 2.3000e+01,  8.7719e+00,  6.5710e+00,  1.1257e+00],\n        [ 2.3000e+01,  7.5162e+00,  8.6071e+00, -5.5526e+00],\n        [ 2.3000e+01, -4.9842e+00,  8.7528e+00,  7.3109e+00],\n        [ 2.3000e+01, -3.8060e+00,  1.0816e+01,  8.0936e+00],\n        [ 2.3000e+01, -3.8484e+00,  1.0858e+01, -2.6079e+00],\n        [ 2.3000e+01,  2.4066e+00,  9.9451e-02,  4.1944e+00],\n        [ 2.3000e+01, -3.7534e+00,  1.0883e+01,  5.5210e+00],\n        [ 2.3000e+01,  1.2828e+00,  2.1490e+00,  3.5953e+00],\n        [ 2.3000e+01,  1.1600e+00,  2.3502e+00,  6.2059e+00],\n        [ 2.3000e+01,  1.3622e+00,  2.2789e+00,  8.7462e+00],\n        [ 2.3000e+01, -7.5781e-02,  4.5331e+00, -2.5778e+00],\n        [ 2.3000e+01, -1.3146e-01,  4.3342e+00,  4.8638e-03],\n        [ 2.3000e+01, -1.3475e+00,  6.5144e+00,  9.7532e+00],\n        [ 2.3000e+01, -1.3790e+00,  6.4477e+00, -9.5860e-01],\n        [ 2.3000e+01, -1.3640e+00,  6.2347e+00,  1.5263e+00],\n        [ 2.3000e+01, -1.3943e+00,  6.6350e+00,  7.2270e+00],\n        [ 2.3000e+01, -2.6133e+00,  8.7412e+00,  8.9831e+00],\n        [ 2.3000e+01, -2.6961e+00,  8.6969e+00, -1.8769e+00],\n        [ 2.3000e+01, -2.5017e+00,  8.9338e+00,  3.8203e+00],\n        [ 2.3000e+01, -2.5667e+00,  8.7896e+00,  6.3988e+00],\n        [ 2.3000e+01, -1.2519e+00,  1.0824e+01, -6.0595e+00],\n        [ 2.3000e+01,  5.0808e+00,  1.8648e-02,  8.8138e-01],\n        [ 2.3000e+01,  4.9476e+00,  1.3566e-01,  3.4415e+00],\n        [ 2.3000e+01,  3.7722e+00,  2.2114e+00,  2.7638e+00],\n        [ 2.3000e+01,  3.8700e+00,  2.2503e+00,  7.8596e+00],\n        [ 2.3000e+01,  2.3455e+00,  4.6263e+00,  9.7133e+00],\n        [ 2.3000e+01,  2.4062e+00,  4.5797e+00, -7.7441e-01],\n        [ 2.3000e+01,  2.5784e+00,  4.3002e+00,  1.9229e+00],\n        [ 2.3000e+01,  2.5234e+00,  4.4887e+00,  7.0368e+00],\n        [ 2.3000e+01,  1.3030e+00,  6.4104e+00, -1.7962e+00],\n        [ 2.3000e+01,  1.4267e+00,  6.4792e+00,  3.7847e+00],\n        [ 2.3000e+01,  1.2748e+00,  6.5028e+00,  6.4278e+00],\n        [ 2.3000e+01,  1.4666e-01,  8.8371e+00,  1.4248e-01],\n        [ 2.3000e+01,  2.9344e-02,  8.7019e+00,  2.5317e+00],\n        [ 2.3000e+01,  1.4088e+00,  1.0888e+01,  8.4981e-01],\n        [ 2.3000e+01,  6.4077e+00,  2.0303e+00, -9.0608e-01],\n        [ 2.3000e+01,  6.2521e+00,  2.2974e+00,  1.7827e+00],\n        [ 2.3000e+01,  6.3490e+00,  2.2838e+00,  4.4197e+00],\n        [ 2.3000e+01,  6.3132e+00,  2.4440e+00,  7.1404e+00],\n        [ 2.3000e+01,  5.0498e+00,  4.3563e+00,  3.5782e+00],\n        [ 2.3000e+01,  5.0688e+00,  4.4946e+00,  6.1092e+00],\n        [ 2.3000e+01,  3.6740e+00,  6.6287e+00,  8.0330e+00],\n        [ 2.3000e+01,  3.7270e+00,  6.5517e+00, -2.6775e+00],\n        [ 2.3000e+01,  3.8088e+00,  6.3555e+00, -5.3497e-02],\n        [ 2.3000e+01,  3.7833e+00,  6.6091e+00,  5.4973e+00],\n        [ 2.3000e+01,  2.5713e+00,  8.7025e+00, -5.9411e+00],\n        [ 2.3000e+01,  2.5017e+00,  8.7175e+00, -8.9264e-01],\n        [ 2.3000e+01,  2.3606e+00,  8.9141e+00,  2.0308e+00],\n        [ 2.3000e+01,  1.0046e+01,  5.2505e-02,  1.6647e+00],\n        [ 2.3000e+01,  9.9669e+00,  1.5159e-01,  4.3249e+00],\n        [ 2.3000e+01,  1.0157e+01,  1.8834e-01,  7.1288e+00],\n        [ 2.3000e+01,  8.8438e+00,  2.2589e+00, -1.6157e+00],\n        [ 2.3000e+01,  8.8027e+00,  2.1762e+00,  9.3300e-01],\n        [ 2.3000e+01,  8.8793e+00,  2.3602e+00,  3.5169e+00],\n        [ 2.3000e+01,  7.6104e+00,  4.2379e+00, -2.7372e+00],\n        [ 2.3000e+01,  7.6143e+00,  4.3794e+00,  2.7575e+00],\n        [ 2.3000e+01,  7.5744e+00,  4.4722e+00,  5.3377e+00],\n        [ 2.3000e+01,  6.2976e+00,  6.5272e+00,  7.1292e+00],\n        [ 2.3000e+01,  6.3079e+00,  6.4506e+00, -3.4636e+00],\n        [ 2.3000e+01,  6.2489e+00,  6.6003e+00, -8.0817e-01],\n        [ 2.3000e+01,  6.3716e+00,  6.5278e+00,  4.5388e+00],\n        [ 2.3000e+01,  5.1207e+00,  8.5235e+00, -6.8987e+00],\n        [ 2.3000e+01,  4.8895e+00,  8.7183e+00, -4.4129e+00],\n        [ 2.4000e+01,  1.1208e+01,  2.2071e+00, -2.7342e+00],\n        [ 2.4000e+01,  1.0076e+01,  4.2684e+00, -3.5428e+00],\n        [ 2.4000e+01,  9.9965e+00,  4.4821e+00,  4.4442e+00],\n        [ 2.4000e+01,  8.8223e+00,  6.4946e+00,  3.6211e+00],\n        [ 2.4000e+01,  7.5334e+00,  8.7286e+00,  5.3763e+00],\n        [ 2.4000e+01,  5.0608e+00,  2.0514e-01,  8.9415e+00],\n        [ 2.4000e+01,  2.5168e+00,  4.4337e+00,  4.4770e+00],\n        [ 2.4000e+01,  1.2502e+00,  6.6235e+00,  8.9006e+00],\n        [ 2.4000e+01,  7.6151e+00,  1.1651e-03,  2.3352e-03],\n        [ 2.4000e+01,  6.3463e+00,  2.1420e+00,  9.8058e+00],\n        [ 2.4000e+01,  5.0582e+00,  4.4144e+00,  1.0811e+00],\n        [ 2.4000e+01,  2.5542e+00,  8.7086e+00, -3.5664e+00],\n        [ 2.4000e+01,  9.9372e+00,  2.1468e-02, -7.4761e-01],\n        [ 2.4000e+01,  7.6382e+00,  4.4388e+00,  7.9015e+00],\n        [ 7.4000e+01, -5.0086e+00,  8.7917e+00,  4.6004e+00],\n        [ 7.4000e+01,  1.0800e+00,  2.1437e+00,  1.0248e+00],\n        [ 7.4000e+01, -1.0935e-02,  4.3574e+00,  5.3976e+00],\n        [ 7.4000e+01,  3.7836e+00,  2.3327e+00,  5.2800e+00],\n        [ 7.4000e+01,  2.5233e-02,  8.6950e+00, -5.2400e+00],\n        [ 7.4000e+01,  6.1931e+00,  1.0933e+01,  4.5458e+00],\n        [ 7.4000e+01,  6.3392e+00,  6.4710e+00,  1.8929e+00],\n        [ 7.4000e+01,  4.8821e+00,  8.7523e+00, -1.7877e+00]],\n       grad_fn=<AddBackward0>), 'mol_nbrs': ([tensor([[  0,   0],\n        [  0,   0],\n        [  0,   0],\n        ...,\n        [123, 123],\n        [123, 123],\n        [123, 123]])], [tensor([[  0,   0],\n        [  0,   1],\n        [  0,   2],\n        ...,\n        [123, 121],\n        [123, 122],\n        [123, 123]])], [tensor([40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 22, 22, 22, 22, 22, 22, 22, 22,\n        22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24,\n        24, 24, 24, 24, 24, 24, 24, 24, 74, 74, 74, 74, 74, 74, 74, 74])], [343], [tensor([[-2.3394e-02,  1.8397e-01,  2.5179e+01],\n        [-4.6789e-02,  3.6793e-01,  5.0357e+01],\n        [-7.0183e-02,  5.5190e-01,  7.5536e+01],\n        ...,\n        [ 7.0183e-02, -5.5190e-01, -7.5536e+01],\n        [ 4.6789e-02, -3.6793e-01, -5.0357e+01],\n        [ 2.3394e-02, -1.8397e-01, -2.5179e+01]], dtype=torch.float64)], [tensor([False,  True,  True,  ...,  True,  True,  True])]), 'mol_idx': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0]), 'delta': tensor([[ 0.0109,  0.0035, -0.0055],\n        [-0.0260, -0.0031,  0.0036],\n        [-0.0020, -0.0011, -0.0016],\n        [-0.0015,  0.0105,  0.0004],\n        [-0.0106,  0.0028, -0.0033],\n        [-0.0082, -0.0040,  0.0126],\n        [-0.0019, -0.0115,  0.0094],\n        [ 0.0105,  0.0082,  0.0085],\n        [ 0.0155, -0.0135, -0.0047],\n        [ 0.0364, -0.0011,  0.0003],\n        [ 0.0112, -0.0164,  0.0088],\n        [ 0.0111, -0.0052, -0.0031],\n        [ 0.0008, -0.0015,  0.0137],\n        [-0.0062, -0.0053, -0.0048],\n        [-0.0119, -0.0037, -0.0190],\n        [ 0.0018, -0.0105,  0.0069],\n        [ 0.0007, -0.0069,  0.0081],\n        [-0.0013, -0.0153, -0.0056],\n        [-0.0200, -0.0036,  0.0079],\n        [-0.0142, -0.0077, -0.0085],\n        [ 0.0026,  0.0069,  0.0057],\n        [-0.0077,  0.0213, -0.0050],\n        [ 0.0074,  0.0017,  0.0002],\n        [-0.0003,  0.0073,  0.0005],\n        [-0.0153, -0.0111,  0.0106],\n        [-0.0002, -0.0047, -0.0145],\n        [-0.0004, -0.0053,  0.0149],\n        [-0.0084,  0.0144, -0.0016],\n        [ 0.0013, -0.0039, -0.0091],\n        [-0.0132, -0.0169,  0.0059],\n        [-0.0010,  0.0216,  0.0062],\n        [ 0.0005, -0.0002, -0.0045],\n        [ 0.0015,  0.0088, -0.0195],\n        [-0.0062, -0.0068,  0.0104],\n        [-0.0059,  0.0076, -0.0012],\n        [-0.0137, -0.0138,  0.0057],\n        [-0.0154, -0.0019,  0.0023],\n        [-0.0120,  0.0024, -0.0053],\n        [-0.0110, -0.0075, -0.0067],\n        [ 0.0013,  0.0009, -0.0040],\n        [ 0.0018, -0.0083,  0.0044],\n        [-0.0165,  0.0121,  0.0165],\n        [-0.0148, -0.0042, -0.0318],\n        [-0.0069,  0.0092,  0.0064],\n        [ 0.0005,  0.0006,  0.0026],\n        [ 0.0069, -0.0008, -0.0086],\n        [ 0.0271, -0.0188, -0.0096],\n        [-0.0034,  0.0140,  0.0037],\n        [-0.0118, -0.0002,  0.0080],\n        [ 0.0159, -0.0013,  0.0080],\n        [-0.0130, -0.0084, -0.0033],\n        [-0.0026,  0.0085,  0.0028],\n        [ 0.0029,  0.0046, -0.0196],\n        [-0.0068, -0.0158, -0.0031],\n        [-0.0083, -0.0005, -0.0135],\n        [-0.0332, -0.0114,  0.0066],\n        [ 0.0085, -0.0180, -0.0026],\n        [-0.0008, -0.0083, -0.0099],\n        [-0.0160,  0.0019,  0.0090],\n        [ 0.0066, -0.0043,  0.0117],\n        [ 0.0016,  0.0022, -0.0091],\n        [-0.0007,  0.0054,  0.0162],\n        [-0.0061,  0.0110,  0.0141],\n        [-0.0032,  0.0023,  0.0005],\n        [ 0.0034,  0.0128,  0.0029],\n        [-0.0072,  0.0072,  0.0091],\n        [-0.0012, -0.0108, -0.0117],\n        [ 0.0019,  0.0023, -0.0133],\n        [ 0.0032, -0.0041, -0.0044],\n        [-0.0063,  0.0099,  0.0021],\n        [ 0.0072, -0.0059,  0.0160],\n        [-0.0101, -0.0019,  0.0035],\n        [-0.0151,  0.0153,  0.0030],\n        [ 0.0174, -0.0071, -0.0176],\n        [-0.0058, -0.0043, -0.0091],\n        [-0.0176, -0.0201, -0.0063],\n        [ 0.0050,  0.0005,  0.0056],\n        [-0.0152, -0.0013,  0.0209],\n        [-0.0167, -0.0212, -0.0094],\n        [-0.0134,  0.0040, -0.0067],\n        [-0.0028, -0.0175,  0.0036],\n        [ 0.0053,  0.0009,  0.0065],\n        [-0.0010, -0.0119, -0.0052],\n        [ 0.0207, -0.0131,  0.0243],\n        [-0.0066,  0.0105, -0.0191],\n        [ 0.0016,  0.0088,  0.0188],\n        [-0.0113, -0.0032, -0.0016],\n        [-0.0088,  0.0159, -0.0113],\n        [-0.0145,  0.0005, -0.0200],\n        [-0.0062, -0.0061, -0.0049],\n        [-0.0115,  0.0101,  0.0075],\n        [ 0.0044,  0.0102, -0.0126],\n        [ 0.0076,  0.0001,  0.0009],\n        [-0.0037, -0.0005,  0.0037],\n        [ 0.0147, -0.0138,  0.0034],\n        [-0.0033,  0.0063,  0.0064],\n        [-0.0153, -0.0061,  0.0042],\n        [-0.0003,  0.0026,  0.0022],\n        [-0.0236, -0.0104,  0.0117],\n        [-0.0039, -0.0106, -0.0039],\n        [ 0.0186,  0.0069,  0.0105],\n        [-0.0055, -0.0010, -0.0156],\n        [-0.0041,  0.0093, -0.0100],\n        [ 0.0033,  0.0150,  0.0087],\n        [ 0.0113, -0.0056, -0.0012],\n        [-0.0229, -0.0337, -0.0178],\n        [ 0.0200,  0.0007, -0.0183],\n        [ 0.0084, -0.0072,  0.0025],\n        [-0.0075, -0.0175, -0.0136],\n        [ 0.0125, -0.0038, -0.0005],\n        [-0.0005, -0.0003,  0.0021],\n        [-0.0145, -0.0241,  0.0201],\n        [ 0.0045,  0.0034, -0.0053],\n        [ 0.0173,  0.0019, -0.0013],\n        [ 0.0089,  0.0135,  0.0144],\n        [ 0.0059, -0.0202, -0.0080],\n        [-0.0060, -0.0037,  0.0029],\n        [-0.0065, -0.0050, -0.0020],\n        [-0.0163,  0.0006,  0.0031],\n        [ 0.0075,  0.0068,  0.0136],\n        [ 0.0091, -0.0066,  0.0014],\n        [-0.0007,  0.0136,  0.0045],\n        [-0.0062,  0.0004,  0.0155],\n        [-0.0122,  0.0038, -0.0028]], requires_grad=True)}\nDeclaration: forward(__torch__.mace.modules.models.ScaleShiftMACE self, Dict(str, Tensor) data, bool training=False, bool compute_force=True, bool compute_virials=False, bool compute_stress=False, bool compute_displacement=False, bool compute_hessian=False) -> Dict(str, Tensor?)\nCast error details: Unable to cast ([tensor([[  0,   0],\n        [  0,   0],\n        [  0,   0],\n        ...,\n        [123, 123],\n        [123, 123],\n        [123, 123]])], [tensor([[  0,   0],\n        [  0,   1],\n        [  0,   2],\n        ...,\n        [123, 121],\n        [123, 122],\n        [123, 123]])], [tensor([40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 22, 22, 22, 22, 22, 22, 22, 22,\n        22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24,\n        24, 24, 24, 24, 24, 24, 24, 24, 74, 74, 74, 74, 74, 74, 74, 74])], [343], [tensor([[-2.3394e-02,  1.8397e-01,  2.5179e+01],\n        [-4.6789e-02,  3.6793e-01,  5.0357e+01],\n        [-7.0183e-02,  5.5190e-01,  7.5536e+01],\n        ...,\n        [ 7.0183e-02, -5.5190e-01, -7.5536e+01],\n        [ 4.6789e-02, -3.6793e-01, -5.0357e+01],\n        [ 2.3394e-02, -1.8397e-01, -2.5179e+01]], dtype=torch.float64)], [tensor([False,  True,  True,  ...,  True,  True,  True])]) to Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Packages/active_learning_mlips/robust/schnet/attacker.py:50\u001b[0m, in \u001b[0;36mAttacker.attack\u001b[0;34m(self, lattice, epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 50\u001b[0m     epoch_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlattice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mepoch_results\n\u001b[1;32m     54\u001b[0m     })\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Packages/active_learning_mlips/robust/schnet/attacker.py:65\u001b[0m, in \u001b[0;36mAttacker.attack_epoch\u001b[0;34m(self, opt, delta, epoch, lattice)\u001b[0m\n\u001b[1;32m     59\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     61\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_attack(delta,\n\u001b[1;32m     62\u001b[0m                             update_nbr_list\u001b[38;5;241m=\u001b[39m(epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbr_list_update \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     63\u001b[0m                             lattice\u001b[38;5;241m=\u001b[39mlattice)\n\u001b[0;32m---> 65\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     70\u001b[0m forces \u001b[38;5;241m=\u001b[39m ch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m     71\u001b[0m     r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_grad\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m     73\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lattice:\n",
      "File \u001b[0;32m~/Packages/active_learning_mlips/robust/schnet/attacker.py:66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     61\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_attack(delta,\n\u001b[1;32m     62\u001b[0m                             update_nbr_list\u001b[38;5;241m=\u001b[39m(epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbr_list_update \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     63\u001b[0m                             lattice\u001b[38;5;241m=\u001b[39mlattice)\n\u001b[1;32m     65\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensemble\u001b[38;5;241m.\u001b[39mmodels\n\u001b[1;32m     68\u001b[0m ]\n\u001b[1;32m     70\u001b[0m forces \u001b[38;5;241m=\u001b[39m ch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m     71\u001b[0m     r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_grad\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m     73\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lattice:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nff/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nff/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: forward() Expected a value of type 'Dict[str, Tensor]' for argument 'data' but instead found type 'dict'.\nPosition: 1\nValue: {'energy': tensor(0.), 'energy_grad': tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]), 'stress': tensor([0., 0., 0., 0., 0., 0.]), 'num_atoms': tensor([124]), 'nbr_list': tensor([[  0,   1],\n        [  0,   3],\n        [  0,   4],\n        ...,\n        [121, 122],\n        [121, 123],\n        [122, 123]]), 'offsets': tensor(indices=tensor([[    0,     0,     0,  ..., 12900, 12900, 12900],\n                       [    0,     1,     2,  ...,     0,     1,     2]]),\n       values=tensor([ 6.3086, 10.8680, -8.8094,  ...,  6.3086, 10.8680,\n                      -8.8094]),\n       size=(12904, 3), nnz=22692, layout=torch.sparse_coo), 'cell': tensor([[ 1.2612e+01, -3.6813e-02, -4.4531e+00],\n        [-6.3037e+00,  1.0905e+01, -4.3563e+00],\n        [-1.2380e-02,  9.7350e-02,  1.3324e+01]]), 'lattice': tensor([[ 1.2612e+01, -3.6813e-02, -4.4531e+00],\n        [-6.3037e+00,  1.0905e+01, -4.3563e+00],\n        [-1.2380e-02,  9.7350e-02,  1.3324e+01]], dtype=torch.float64), 'nxyz': tensor([[ 4.0000e+01,  7.4146e+00,  8.7504e+00, -2.7836e+00],\n        [ 4.0000e+01,  2.4671e+00,  1.4621e-01,  7.0905e+00],\n        [ 4.0000e+01,  1.2476e+00,  2.3217e+00,  1.1514e+01],\n        [ 4.0000e+01, -1.3462e+00,  6.5356e+00,  4.3810e+00],\n        [ 4.0000e+01, -2.4706e+00,  8.6695e+00,  8.2488e-01],\n        [ 4.0000e+01,  3.7842e+00,  2.2125e+00,  5.8137e-02],\n        [ 4.0000e+01, -2.5721e-02,  8.6543e+00, -2.4998e+00],\n        [ 4.0000e+01,  1.9152e-01,  8.8615e+00,  5.3019e+00],\n        [ 4.0000e+01,  5.0605e+00,  4.4237e+00,  8.8908e+00],\n        [ 4.0000e+01,  4.9347e+00,  8.7714e+00,  9.8331e-01],\n        [ 2.2000e+01,  6.1132e+00,  1.0829e+01, -6.1018e+00],\n        [ 2.2000e+01, -1.2548e+00,  2.3091e+00,  9.7359e+00],\n        [ 2.2000e+01,  8.7753e+00,  6.4054e+00, -1.7335e+00],\n        [ 2.2000e+01, -2.1754e-02,  4.2954e+00,  2.6812e+00],\n        [ 2.2000e+01, -8.2604e-02,  4.4695e+00,  8.0474e+00],\n        [ 2.2000e+01,  5.2318e+00,  1.4455e-01,  6.1814e+00],\n        [ 2.2000e+01,  3.9508e+00,  2.1815e+00,  1.0501e+01],\n        [ 2.2000e+01,  1.2982e+00,  6.5365e+00,  9.8642e-01],\n        [ 2.2000e+01,  7.6722e+00,  1.4307e-01, -2.5230e+00],\n        [ 2.2000e+01,  7.5634e+00,  1.4962e-01,  2.5938e+00],\n        [ 2.2000e+01,  7.4822e+00,  1.4709e-01,  7.8721e+00],\n        [ 2.2000e+01,  5.0839e+00,  4.4377e+00, -1.5865e+00],\n        [ 2.2000e+01,  3.7637e+00,  6.5361e+00,  2.7470e+00],\n        [ 2.2000e+01,  2.9994e+00,  9.3758e+00,  4.8114e+00],\n        [ 2.2000e+01,  8.8663e+00,  2.1270e+00, -4.3513e+00],\n        [ 2.2000e+01,  8.7727e+00,  2.2294e+00,  6.1476e+00],\n        [ 2.2000e+01,  7.5389e+00,  4.3442e+00,  7.1980e-02],\n        [ 2.2000e+01,  5.1560e+00,  8.6788e+00,  3.7544e+00],\n        [ 2.3000e+01,  1.2409e+01,  1.8510e-01,  8.6290e-01],\n        [ 2.3000e+01,  1.2452e+01,  8.3813e-02,  3.6087e+00],\n        [ 2.3000e+01,  6.3092e+00,  1.0939e+01,  1.9948e+00],\n        [ 2.3000e+01,  1.1303e+01,  2.2009e+00, -5.3179e+00],\n        [ 2.3000e+01,  1.1277e+01,  2.1472e+00, -4.8324e-02],\n        [ 2.3000e+01,  1.1282e+01,  2.1815e+00,  2.6107e+00],\n        [ 2.3000e+01,  9.9991e+00,  4.4390e+00,  7.1078e+00],\n        [ 2.3000e+01,  1.0019e+01,  4.1195e+00, -9.1255e-01],\n        [ 2.3000e+01,  1.0034e+01,  4.3643e+00,  1.8698e+00],\n        [ 2.3000e+01,  8.7705e+00,  6.6091e+00,  6.1481e+00],\n        [ 2.3000e+01,  8.7144e+00,  6.2790e+00, -4.5387e+00],\n        [ 2.3000e+01,  8.7719e+00,  6.5710e+00,  1.1257e+00],\n        [ 2.3000e+01,  7.5162e+00,  8.6071e+00, -5.5526e+00],\n        [ 2.3000e+01, -4.9842e+00,  8.7528e+00,  7.3109e+00],\n        [ 2.3000e+01, -3.8060e+00,  1.0816e+01,  8.0936e+00],\n        [ 2.3000e+01, -3.8484e+00,  1.0858e+01, -2.6079e+00],\n        [ 2.3000e+01,  2.4066e+00,  9.9451e-02,  4.1944e+00],\n        [ 2.3000e+01, -3.7534e+00,  1.0883e+01,  5.5210e+00],\n        [ 2.3000e+01,  1.2828e+00,  2.1490e+00,  3.5953e+00],\n        [ 2.3000e+01,  1.1600e+00,  2.3502e+00,  6.2059e+00],\n        [ 2.3000e+01,  1.3622e+00,  2.2789e+00,  8.7462e+00],\n        [ 2.3000e+01, -7.5781e-02,  4.5331e+00, -2.5778e+00],\n        [ 2.3000e+01, -1.3146e-01,  4.3342e+00,  4.8638e-03],\n        [ 2.3000e+01, -1.3475e+00,  6.5144e+00,  9.7532e+00],\n        [ 2.3000e+01, -1.3790e+00,  6.4477e+00, -9.5860e-01],\n        [ 2.3000e+01, -1.3640e+00,  6.2347e+00,  1.5263e+00],\n        [ 2.3000e+01, -1.3943e+00,  6.6350e+00,  7.2270e+00],\n        [ 2.3000e+01, -2.6133e+00,  8.7412e+00,  8.9831e+00],\n        [ 2.3000e+01, -2.6961e+00,  8.6969e+00, -1.8769e+00],\n        [ 2.3000e+01, -2.5017e+00,  8.9338e+00,  3.8203e+00],\n        [ 2.3000e+01, -2.5667e+00,  8.7896e+00,  6.3988e+00],\n        [ 2.3000e+01, -1.2519e+00,  1.0824e+01, -6.0595e+00],\n        [ 2.3000e+01,  5.0808e+00,  1.8648e-02,  8.8138e-01],\n        [ 2.3000e+01,  4.9476e+00,  1.3566e-01,  3.4415e+00],\n        [ 2.3000e+01,  3.7722e+00,  2.2114e+00,  2.7638e+00],\n        [ 2.3000e+01,  3.8700e+00,  2.2503e+00,  7.8596e+00],\n        [ 2.3000e+01,  2.3455e+00,  4.6263e+00,  9.7133e+00],\n        [ 2.3000e+01,  2.4062e+00,  4.5797e+00, -7.7441e-01],\n        [ 2.3000e+01,  2.5784e+00,  4.3002e+00,  1.9229e+00],\n        [ 2.3000e+01,  2.5234e+00,  4.4887e+00,  7.0368e+00],\n        [ 2.3000e+01,  1.3030e+00,  6.4104e+00, -1.7962e+00],\n        [ 2.3000e+01,  1.4267e+00,  6.4792e+00,  3.7847e+00],\n        [ 2.3000e+01,  1.2748e+00,  6.5028e+00,  6.4278e+00],\n        [ 2.3000e+01,  1.4666e-01,  8.8371e+00,  1.4248e-01],\n        [ 2.3000e+01,  2.9344e-02,  8.7019e+00,  2.5317e+00],\n        [ 2.3000e+01,  1.4088e+00,  1.0888e+01,  8.4981e-01],\n        [ 2.3000e+01,  6.4077e+00,  2.0303e+00, -9.0608e-01],\n        [ 2.3000e+01,  6.2521e+00,  2.2974e+00,  1.7827e+00],\n        [ 2.3000e+01,  6.3490e+00,  2.2838e+00,  4.4197e+00],\n        [ 2.3000e+01,  6.3132e+00,  2.4440e+00,  7.1404e+00],\n        [ 2.3000e+01,  5.0498e+00,  4.3563e+00,  3.5782e+00],\n        [ 2.3000e+01,  5.0688e+00,  4.4946e+00,  6.1092e+00],\n        [ 2.3000e+01,  3.6740e+00,  6.6287e+00,  8.0330e+00],\n        [ 2.3000e+01,  3.7270e+00,  6.5517e+00, -2.6775e+00],\n        [ 2.3000e+01,  3.8088e+00,  6.3555e+00, -5.3497e-02],\n        [ 2.3000e+01,  3.7833e+00,  6.6091e+00,  5.4973e+00],\n        [ 2.3000e+01,  2.5713e+00,  8.7025e+00, -5.9411e+00],\n        [ 2.3000e+01,  2.5017e+00,  8.7175e+00, -8.9264e-01],\n        [ 2.3000e+01,  2.3606e+00,  8.9141e+00,  2.0308e+00],\n        [ 2.3000e+01,  1.0046e+01,  5.2505e-02,  1.6647e+00],\n        [ 2.3000e+01,  9.9669e+00,  1.5159e-01,  4.3249e+00],\n        [ 2.3000e+01,  1.0157e+01,  1.8834e-01,  7.1288e+00],\n        [ 2.3000e+01,  8.8438e+00,  2.2589e+00, -1.6157e+00],\n        [ 2.3000e+01,  8.8027e+00,  2.1762e+00,  9.3300e-01],\n        [ 2.3000e+01,  8.8793e+00,  2.3602e+00,  3.5169e+00],\n        [ 2.3000e+01,  7.6104e+00,  4.2379e+00, -2.7372e+00],\n        [ 2.3000e+01,  7.6143e+00,  4.3794e+00,  2.7575e+00],\n        [ 2.3000e+01,  7.5744e+00,  4.4722e+00,  5.3377e+00],\n        [ 2.3000e+01,  6.2976e+00,  6.5272e+00,  7.1292e+00],\n        [ 2.3000e+01,  6.3079e+00,  6.4506e+00, -3.4636e+00],\n        [ 2.3000e+01,  6.2489e+00,  6.6003e+00, -8.0817e-01],\n        [ 2.3000e+01,  6.3716e+00,  6.5278e+00,  4.5388e+00],\n        [ 2.3000e+01,  5.1207e+00,  8.5235e+00, -6.8987e+00],\n        [ 2.3000e+01,  4.8895e+00,  8.7183e+00, -4.4129e+00],\n        [ 2.4000e+01,  1.1208e+01,  2.2071e+00, -2.7342e+00],\n        [ 2.4000e+01,  1.0076e+01,  4.2684e+00, -3.5428e+00],\n        [ 2.4000e+01,  9.9965e+00,  4.4821e+00,  4.4442e+00],\n        [ 2.4000e+01,  8.8223e+00,  6.4946e+00,  3.6211e+00],\n        [ 2.4000e+01,  7.5334e+00,  8.7286e+00,  5.3763e+00],\n        [ 2.4000e+01,  5.0608e+00,  2.0514e-01,  8.9415e+00],\n        [ 2.4000e+01,  2.5168e+00,  4.4337e+00,  4.4770e+00],\n        [ 2.4000e+01,  1.2502e+00,  6.6235e+00,  8.9006e+00],\n        [ 2.4000e+01,  7.6151e+00,  1.1651e-03,  2.3352e-03],\n        [ 2.4000e+01,  6.3463e+00,  2.1420e+00,  9.8058e+00],\n        [ 2.4000e+01,  5.0582e+00,  4.4144e+00,  1.0811e+00],\n        [ 2.4000e+01,  2.5542e+00,  8.7086e+00, -3.5664e+00],\n        [ 2.4000e+01,  9.9372e+00,  2.1468e-02, -7.4761e-01],\n        [ 2.4000e+01,  7.6382e+00,  4.4388e+00,  7.9015e+00],\n        [ 7.4000e+01, -5.0086e+00,  8.7917e+00,  4.6004e+00],\n        [ 7.4000e+01,  1.0800e+00,  2.1437e+00,  1.0248e+00],\n        [ 7.4000e+01, -1.0935e-02,  4.3574e+00,  5.3976e+00],\n        [ 7.4000e+01,  3.7836e+00,  2.3327e+00,  5.2800e+00],\n        [ 7.4000e+01,  2.5233e-02,  8.6950e+00, -5.2400e+00],\n        [ 7.4000e+01,  6.1931e+00,  1.0933e+01,  4.5458e+00],\n        [ 7.4000e+01,  6.3392e+00,  6.4710e+00,  1.8929e+00],\n        [ 7.4000e+01,  4.8821e+00,  8.7523e+00, -1.7877e+00]],\n       grad_fn=<AddBackward0>), 'mol_nbrs': ([tensor([[  0,   0],\n        [  0,   0],\n        [  0,   0],\n        ...,\n        [123, 123],\n        [123, 123],\n        [123, 123]])], [tensor([[  0,   0],\n        [  0,   1],\n        [  0,   2],\n        ...,\n        [123, 121],\n        [123, 122],\n        [123, 123]])], [tensor([40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 22, 22, 22, 22, 22, 22, 22, 22,\n        22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24,\n        24, 24, 24, 24, 24, 24, 24, 24, 74, 74, 74, 74, 74, 74, 74, 74])], [343], [tensor([[-2.3394e-02,  1.8397e-01,  2.5179e+01],\n        [-4.6789e-02,  3.6793e-01,  5.0357e+01],\n        [-7.0183e-02,  5.5190e-01,  7.5536e+01],\n        ...,\n        [ 7.0183e-02, -5.5190e-01, -7.5536e+01],\n        [ 4.6789e-02, -3.6793e-01, -5.0357e+01],\n        [ 2.3394e-02, -1.8397e-01, -2.5179e+01]], dtype=torch.float64)], [tensor([False,  True,  True,  ...,  True,  True,  True])]), 'mol_idx': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0]), 'delta': tensor([[ 0.0109,  0.0035, -0.0055],\n        [-0.0260, -0.0031,  0.0036],\n        [-0.0020, -0.0011, -0.0016],\n        [-0.0015,  0.0105,  0.0004],\n        [-0.0106,  0.0028, -0.0033],\n        [-0.0082, -0.0040,  0.0126],\n        [-0.0019, -0.0115,  0.0094],\n        [ 0.0105,  0.0082,  0.0085],\n        [ 0.0155, -0.0135, -0.0047],\n        [ 0.0364, -0.0011,  0.0003],\n        [ 0.0112, -0.0164,  0.0088],\n        [ 0.0111, -0.0052, -0.0031],\n        [ 0.0008, -0.0015,  0.0137],\n        [-0.0062, -0.0053, -0.0048],\n        [-0.0119, -0.0037, -0.0190],\n        [ 0.0018, -0.0105,  0.0069],\n        [ 0.0007, -0.0069,  0.0081],\n        [-0.0013, -0.0153, -0.0056],\n        [-0.0200, -0.0036,  0.0079],\n        [-0.0142, -0.0077, -0.0085],\n        [ 0.0026,  0.0069,  0.0057],\n        [-0.0077,  0.0213, -0.0050],\n        [ 0.0074,  0.0017,  0.0002],\n        [-0.0003,  0.0073,  0.0005],\n        [-0.0153, -0.0111,  0.0106],\n        [-0.0002, -0.0047, -0.0145],\n        [-0.0004, -0.0053,  0.0149],\n        [-0.0084,  0.0144, -0.0016],\n        [ 0.0013, -0.0039, -0.0091],\n        [-0.0132, -0.0169,  0.0059],\n        [-0.0010,  0.0216,  0.0062],\n        [ 0.0005, -0.0002, -0.0045],\n        [ 0.0015,  0.0088, -0.0195],\n        [-0.0062, -0.0068,  0.0104],\n        [-0.0059,  0.0076, -0.0012],\n        [-0.0137, -0.0138,  0.0057],\n        [-0.0154, -0.0019,  0.0023],\n        [-0.0120,  0.0024, -0.0053],\n        [-0.0110, -0.0075, -0.0067],\n        [ 0.0013,  0.0009, -0.0040],\n        [ 0.0018, -0.0083,  0.0044],\n        [-0.0165,  0.0121,  0.0165],\n        [-0.0148, -0.0042, -0.0318],\n        [-0.0069,  0.0092,  0.0064],\n        [ 0.0005,  0.0006,  0.0026],\n        [ 0.0069, -0.0008, -0.0086],\n        [ 0.0271, -0.0188, -0.0096],\n        [-0.0034,  0.0140,  0.0037],\n        [-0.0118, -0.0002,  0.0080],\n        [ 0.0159, -0.0013,  0.0080],\n        [-0.0130, -0.0084, -0.0033],\n        [-0.0026,  0.0085,  0.0028],\n        [ 0.0029,  0.0046, -0.0196],\n        [-0.0068, -0.0158, -0.0031],\n        [-0.0083, -0.0005, -0.0135],\n        [-0.0332, -0.0114,  0.0066],\n        [ 0.0085, -0.0180, -0.0026],\n        [-0.0008, -0.0083, -0.0099],\n        [-0.0160,  0.0019,  0.0090],\n        [ 0.0066, -0.0043,  0.0117],\n        [ 0.0016,  0.0022, -0.0091],\n        [-0.0007,  0.0054,  0.0162],\n        [-0.0061,  0.0110,  0.0141],\n        [-0.0032,  0.0023,  0.0005],\n        [ 0.0034,  0.0128,  0.0029],\n        [-0.0072,  0.0072,  0.0091],\n        [-0.0012, -0.0108, -0.0117],\n        [ 0.0019,  0.0023, -0.0133],\n        [ 0.0032, -0.0041, -0.0044],\n        [-0.0063,  0.0099,  0.0021],\n        [ 0.0072, -0.0059,  0.0160],\n        [-0.0101, -0.0019,  0.0035],\n        [-0.0151,  0.0153,  0.0030],\n        [ 0.0174, -0.0071, -0.0176],\n        [-0.0058, -0.0043, -0.0091],\n        [-0.0176, -0.0201, -0.0063],\n        [ 0.0050,  0.0005,  0.0056],\n        [-0.0152, -0.0013,  0.0209],\n        [-0.0167, -0.0212, -0.0094],\n        [-0.0134,  0.0040, -0.0067],\n        [-0.0028, -0.0175,  0.0036],\n        [ 0.0053,  0.0009,  0.0065],\n        [-0.0010, -0.0119, -0.0052],\n        [ 0.0207, -0.0131,  0.0243],\n        [-0.0066,  0.0105, -0.0191],\n        [ 0.0016,  0.0088,  0.0188],\n        [-0.0113, -0.0032, -0.0016],\n        [-0.0088,  0.0159, -0.0113],\n        [-0.0145,  0.0005, -0.0200],\n        [-0.0062, -0.0061, -0.0049],\n        [-0.0115,  0.0101,  0.0075],\n        [ 0.0044,  0.0102, -0.0126],\n        [ 0.0076,  0.0001,  0.0009],\n        [-0.0037, -0.0005,  0.0037],\n        [ 0.0147, -0.0138,  0.0034],\n        [-0.0033,  0.0063,  0.0064],\n        [-0.0153, -0.0061,  0.0042],\n        [-0.0003,  0.0026,  0.0022],\n        [-0.0236, -0.0104,  0.0117],\n        [-0.0039, -0.0106, -0.0039],\n        [ 0.0186,  0.0069,  0.0105],\n        [-0.0055, -0.0010, -0.0156],\n        [-0.0041,  0.0093, -0.0100],\n        [ 0.0033,  0.0150,  0.0087],\n        [ 0.0113, -0.0056, -0.0012],\n        [-0.0229, -0.0337, -0.0178],\n        [ 0.0200,  0.0007, -0.0183],\n        [ 0.0084, -0.0072,  0.0025],\n        [-0.0075, -0.0175, -0.0136],\n        [ 0.0125, -0.0038, -0.0005],\n        [-0.0005, -0.0003,  0.0021],\n        [-0.0145, -0.0241,  0.0201],\n        [ 0.0045,  0.0034, -0.0053],\n        [ 0.0173,  0.0019, -0.0013],\n        [ 0.0089,  0.0135,  0.0144],\n        [ 0.0059, -0.0202, -0.0080],\n        [-0.0060, -0.0037,  0.0029],\n        [-0.0065, -0.0050, -0.0020],\n        [-0.0163,  0.0006,  0.0031],\n        [ 0.0075,  0.0068,  0.0136],\n        [ 0.0091, -0.0066,  0.0014],\n        [-0.0007,  0.0136,  0.0045],\n        [-0.0062,  0.0004,  0.0155],\n        [-0.0122,  0.0038, -0.0028]], requires_grad=True)}\nDeclaration: forward(__torch__.mace.modules.models.ScaleShiftMACE self, Dict(str, Tensor) data, bool training=False, bool compute_force=True, bool compute_virials=False, bool compute_stress=False, bool compute_displacement=False, bool compute_hessian=False) -> Dict(str, Tensor?)\nCast error details: Unable to cast ([tensor([[  0,   0],\n        [  0,   0],\n        [  0,   0],\n        ...,\n        [123, 123],\n        [123, 123],\n        [123, 123]])], [tensor([[  0,   0],\n        [  0,   1],\n        [  0,   2],\n        ...,\n        [123, 121],\n        [123, 122],\n        [123, 123]])], [tensor([40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 22, 22, 22, 22, 22, 22, 22, 22,\n        22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24,\n        24, 24, 24, 24, 24, 24, 24, 24, 74, 74, 74, 74, 74, 74, 74, 74])], [343], [tensor([[-2.3394e-02,  1.8397e-01,  2.5179e+01],\n        [-4.6789e-02,  3.6793e-01,  5.0357e+01],\n        [-7.0183e-02,  5.5190e-01,  7.5536e+01],\n        ...,\n        [ 7.0183e-02, -5.5190e-01, -7.5536e+01],\n        [ 4.6789e-02, -3.6793e-01, -5.0357e+01],\n        [ 2.3394e-02, -1.8397e-01, -2.5179e+01]], dtype=torch.float64)], [tensor([False,  True,  True,  ...,  True,  True,  True])]) to Tensor"
     ]
    }
   ],
   "source": [
    "results = attacker.attack(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
